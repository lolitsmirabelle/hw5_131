---
title: "homework 5"
output: html_document
date: '2022-05-11'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidymodels)
library(ISLR) 
library(ISLR2) 
library(ggplot2)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR) # for naive bayes
tidymodels_prefer()
```

## Question 1

```{r}
library(janitor)

pokemon_og <- read.csv("~/Downloads/homework-5/data/pokemon.csv")

pokemon <- clean_names(pokemon_og)
```
The clean_names() function cleans names of the data frame and the resulting names are unique and consist only of the _ character, numbers, and letters. It is useful because the data frame is more organized and clean. 

## Question 2 

```{r}
pokemon %>% 
  ggplot(aes(x = type_1)) +
  geom_bar()
```

There are 18 classes. There are a few Pokemon with only a few Pokemon in their class, such as Flying and Fairy. 

```{r}
classes <- c('Bug', 'Fire', 'Grass', 'Normal', 'Water', 'Psychic')

pokemon <- filter(pokemon, type_1 %in% classes)

pokemon$type_1 <- factor(pokemon$type_1)
pokemon$legendary <- factor(pokemon$legendary)
```


## Question 3
```{r}
set.seed(2424)

pokemon_split <- initial_split(pokemon, prop = 0.80,
                                strata = type_1)

pokemon_train <- training(pokemon_split) 

pokemon_test <- testing(pokemon_split)

pokemon_folds <- vfold_cv(pokemon_train, v = 5)
```


## Question 4
```{r}
pokemon_recipe <- recipe(type_1 ~ ., data = pokemon%>%dplyr::select(type_1,hp:legendary))%>%
  step_dummy(c("legendary", "generation"))%>%
  step_center(all_predictors())%>%
  step_scale(all_predictors())
```


## Question 5
```{r}
pokemon_spec <- 
  multinom_reg(penalty = tune(), mixture = 0) %>% 
  set_engine("glmnet")

pokemon_workflow <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(pokemon_spec)

penalty_grid <- grid_regular(penalty(range = c(-5, 5)), levels = 10)

mixture_grid <- grid_regular(mixture(range = c(0, 1)), levels = 10)
```

We will be fitting 500 models. 


## Question 6
```{r}
tune_res <- tune_grid(
  pokemon_workflow,
  resamples = pokemon_folds, 
  grid = penalty_grid
)

autoplot(tune_res)
```

From the graph above, we notice that smaller values produce better accuracy and ROC AUC. 


## Question 7 
```{r}
collect_metrics(tune_res)

best_penalty <- select_best(tune_res, metric = "roc_auc")
best_penalty

pokemon_final <- finalize_workflow(pokemon_workflow, best_penalty)

pokemon_final_fit <- fit(pokemon_final, data = pokemon_train)

augment(pokemon_final_fit, new_data = pokemon_test) %>%
  accuracy(truth = type_1, estimate = .pred_class)
```
We get an accuracy of 32.97% which means that the training data did not have a good performance on the testing set. 


##  Question 8 
```{r}
# overall ROC AUC
augment(pokemon_final_fit, new_data = pokemon_test)%>%
  roc_auc(type_1, estimate = c(.pred_Fire, .pred_Bug, .pred_Water, .pred_Grass, 
                                 .pred_Normal, .pred_Psychic))

# create plots of the different ROC curves, one per level of the outcome
augment(pokemon_final_fit, new_data = pokemon_test) %>%
  roc_curve(type_1, estimate = c(.pred_Fire, .pred_Bug, .pred_Water, .pred_Grass, 
                                 .pred_Normal, .pred_Psychic)) %>%
  autoplot()

# heat map of the confusion matrix
augment(pokemon_final_fit, new_data = pokemon_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type="heatmap")
```

From this assignment, we can conclude that the model did not perform too well. The ROC AUC was not too high, and the higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes. The Pokemon type that was the best at predicting was Bug type. The Pokemon type that was the worst at predicting was Psychic type. 